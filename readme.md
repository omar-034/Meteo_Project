ğŸŒ¦ï¸ Pipeline ETL : Analyse MÃ©tÃ©orologique de Dakar 2025
ğŸ‘¤ Informations Ã‰tudiant
Auteur : [Omar Diop]

Formation : Master 1 IA & Data Science

Institution : Swiss UMEF, Dakar

Projet : Examen Final - Module Cycle de Vie de la DonnÃ©e

ğŸ“Œ PrÃ©sentation du Projet
Ce projet remplace l'examen final et consiste en la mise en place d'un pipeline ETL (Extract, Transform, Load) complet. L'objectif est de rÃ©cupÃ©rer, traiter et stocker les donnÃ©es mÃ©tÃ©orologiques de la ville de Dakar pour l'annÃ©e 2025 afin d'en extraire des indicateurs climatiques pertinents.

ğŸ—ï¸ Architecture du Pipeline
Le projet suit une logique de flux de donnÃ©es standard en entreprise :

Extraction (API) : RÃ©cupÃ©ration des donnÃ©es historiques via l'API Open-Meteo.

Transformation (Python) :

Nettoyage avec Pandas.

Calcul de la colonne `is_rainy` (Feature Engineering).

Chargement (SQL) : Insertion sÃ©curisÃ©e dans PostgreSQL en utilisant le driver Psycopg2.

Analyse : RequÃªtage SQL pour obtenir des statistiques mensuelles.

ğŸ› ï¸ Stack Technique
Langage : Python 3.10+

Librairies : requests, pandas, psycopg2, matplotlib

Base de donnÃ©es : PostgreSQL 14+

Format de sortie : Jupyter Notebook (.ipynb)

ğŸ“‚ Structure des fichiers
Bash
.
â”œâ”€â”€ meteo.ipynb                   # Notebook principal contenant tout le cycle
â”œâ”€â”€ requirements.txt              # Liste des dÃ©pendances Python
â””â”€â”€ README.md                     # Documentation (ce fichier)

ğŸš€ Installation et Utilisation
1. Cloner le projet et installer les dÃ©pendances
Bash
pip install -r requirements.txt
2. Configuration de la base de donnÃ©es
Le notebook se charge de crÃ©er la table automatiquement. Voici la structure utilisÃ©e :

SQL
CREATE TABLE IF NOT EXISTS meteo_dakar_2025 (
    date_day DATE PRIMARY KEY,
    temp_max FLOAT,
    temp_min FLOAT,
    apparent_temp_mean FLOAT,
    precip_sum FLOAT,
    wind_dir_dominant INT,
    sunshine_duration FLOAT,
    is_rainy BOOLEAN
);
3. ExÃ©cution
Lancez le Jupyter Notebook et exÃ©cutez les cellules dans l'ordre pour voir le pipeline s'animer, de l'appel API jusqu'aux graphiques finaux.

ğŸŒ Perspectives de mise en production
Le projet a Ã©tÃ© pensÃ© pour Ãªtre Ã©volutif. Une suite logique serait l'implÃ©mentation d'une API REST (Flask ou FastAPI) pour servir ces donnÃ©es stockÃ©es en SQL Ã  des applications tierces ou des tableaux de bord interactifs.

ğŸ“š RÃ©fÃ©rences & Documentation
API : Open-Meteo Historical Data

Driver : Psycopg2 Documentation

Cours : Cycle de vie des donnÃ©es - Swiss UMEF Dakar.